{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "colab": {
      "name": "homework(1).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Py3 Research",
      "language": "python",
      "name": "py3_research"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eulvfJWl7ueY"
      },
      "source": [
        "# Lab 1\n",
        "\n",
        "\n",
        "## Part 1: Bilingual dictionary induction and unsupervised embedding-based MT (30%)\n",
        "*Note: this homework is based on materials from yandexdataschool [NLP course](https://github.com/yandexdataschool/nlp_course/). Feel free to check this awesome course if you wish to dig deeper.*\n",
        "\n",
        "*Refined by [Nikolay Karpachev](https://www.linkedin.com/in/nikolay-karpachev-b0146a104/)*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fV4rIjxa7uei"
      },
      "source": [
        "**In this homework** **<font color='red'>YOU</font>** will make machine translation system without using parallel corpora, alignment, attention, 100500 depth super-cool recurrent neural network and all that kind superstuff.\n",
        "\n",
        "But even without parallel corpora this system can be good enough (hopefully), in particular for similar languages, e.g. Ukrainian and Russian. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idSYq2GU7uew"
      },
      "source": [
        "### Frament of the Swadesh list for some slavic languages\n",
        "\n",
        "The Swadesh list is a lexicostatistical stuff. It's named after American linguist Morris Swadesh and contains basic lexis. This list are used to define subgroupings of languages, its relatedness.\n",
        "\n",
        "So we can see some kind of word invariance for different Slavic languages.\n",
        "\n",
        "\n",
        "| Russian         | Belorussian              | Ukrainian               | Polish             | Czech                         | Bulgarian            |\n",
        "|-----------------|--------------------------|-------------------------|--------------------|-------------------------------|-----------------------|\n",
        "| женщина         | жанчына, кабета, баба    | жінка                   | kobieta            | žena                          | жена                  |\n",
        "| мужчина         | мужчына                  | чоловік, мужчина        | mężczyzna          | muž                           | мъж                   |\n",
        "| человек         | чалавек                  | людина, чоловік         | człowiek           | člověk                        | човек                 |\n",
        "| ребёнок, дитя   | дзіця, дзіцёнак, немаўля | дитина, дитя            | dziecko            | dítě                          | дете                  |\n",
        "| жена            | жонка                    | дружина, жінка          | żona               | žena, manželka, choť          | съпруга, жена         |\n",
        "| муж             | муж, гаспадар            | чоловiк, муж            | mąż                | muž, manžel, choť             | съпруг, мъж           |\n",
        "| мать, мама      | маці, матка              | мати, матір, неня, мама | matka              | matka, máma, 'стар.' mateř    | майка                 |\n",
        "| отец, тятя      | бацька, тата             | батько, тато, татусь    | ojciec             | otec                          | баща, татко           |\n",
        "| много           | шмат, багата             | багато                  | wiele              | mnoho, hodně                  | много                 |\n",
        "| несколько       | некалькі, колькі         | декілька, кілька        | kilka              | několik, pár, trocha          | няколко               |\n",
        "| другой, иной    | іншы                     | інший                   | inny               | druhý, jiný                   | друг                  |\n",
        "| зверь, животное | жывёла, звер, істота     | тварина, звір           | zwierzę            | zvíře                         | животно               |\n",
        "| рыба            | рыба                     | риба                    | ryba               | ryba                          | риба                  |\n",
        "| птица           | птушка                   | птах, птиця             | ptak               | pták                          | птица                 |\n",
        "| собака, пёс     | сабака                   | собака, пес             | pies               | pes                           | куче, пес             |\n",
        "| вошь            | вош                      | воша                    | wesz               | veš                           | въшка                 |\n",
        "| змея, гад       | змяя                     | змія, гад               | wąż                | had                           | змия                  |\n",
        "| червь, червяк   | чарвяк                   | хробак, черв'як         | robak              | červ                          | червей                |\n",
        "| дерево          | дрэва                    | дерево                  | drzewo             | strom, dřevo                  | дърво                 |\n",
        "| лес             | лес                      | ліс                     | las                | les                           | гора, лес             |\n",
        "| палка           | кій, палка               | палиця                  | patyk, pręt, pałka | hůl, klacek, prut, kůl, pálka | палка, пръчка, бастун |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cNM3_fjr7ue2"
      },
      "source": [
        "But the context distribution of these languages demonstrates even more invariance. And we can use this fact for our for our purposes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YLppwa527ue6"
      },
      "source": [
        "## Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lYBGKAUn7ue_"
      },
      "source": [
        "import gensim\n",
        "import numpy as np\n",
        "from gensim.models import KeyedVectors"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MwGoVhRA7ufP"
      },
      "source": [
        "In this notebook we're going to use pretrained word vectors - FastText (original paper - https://arxiv.org/abs/1607.04606).\n",
        "\n",
        "You can download them from the official [website](https://fasttext.cc/docs/en/crawl-vectors.html). We're going to need embeddings for Russian and Ukrainian languages. Please use word2vec-compatible format (.text)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u1JjQv_97ufT"
      },
      "source": [
        "uk_emb = KeyedVectors.load_word2vec_format(\"/content/drive/My Drive/ml/cc.uk.300.vec\")"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2uHtQrP_MKYV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f962cf6d-b0d3-4a1c-d3c1-00b29129c19c"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ffzuept_7ufd"
      },
      "source": [
        "ru_emb = KeyedVectors.load_word2vec_format(\"/content/drive/My Drive/ml/cc.ru.300.vec\")"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nTkXfT0W7ufk",
        "outputId": "d215d18a-af74-42bb-d2d9-e7155c5592bf"
      },
      "source": [
        "ru_emb.most_similar([ru_emb[\"август\"]], topn=10)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('август', 1.0),\n",
              " ('июль', 0.9383153915405273),\n",
              " ('сентябрь', 0.9240028858184814),\n",
              " ('июнь', 0.9222575426101685),\n",
              " ('октябрь', 0.9095538854598999),\n",
              " ('ноябрь', 0.8930036425590515),\n",
              " ('апрель', 0.8729087114334106),\n",
              " ('декабрь', 0.8652557730674744),\n",
              " ('март', 0.8545796275138855),\n",
              " ('февраль', 0.8401416540145874)]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vdBA8lcg7ufs",
        "outputId": "3c64531d-c054-4cbf-eb9a-79926a7d4b4e"
      },
      "source": [
        "uk_emb.most_similar([uk_emb[\"серпень\"]])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('серпень', 0.9999999403953552),\n",
              " ('липень', 0.9096440076828003),\n",
              " ('вересень', 0.901697039604187),\n",
              " ('червень', 0.8992519378662109),\n",
              " ('жовтень', 0.8810408711433411),\n",
              " ('листопад', 0.8787633776664734),\n",
              " ('квітень', 0.8592804670333862),\n",
              " ('грудень', 0.8586863279342651),\n",
              " ('травень', 0.8408110737800598),\n",
              " ('лютий', 0.8256431818008423)]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_yJvcKXO7uf0",
        "outputId": "2686fe4f-2b7a-4f97-bba0-eb7345279228"
      },
      "source": [
        "ru_emb.most_similar([uk_emb[\"серпень\"]])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Stepashka.com', 0.2757962942123413),\n",
              " ('ЖИЗНИВадим', 0.25203436613082886),\n",
              " ('2Дмитрий', 0.25048112869262695),\n",
              " ('2012Дмитрий', 0.24829231202602386),\n",
              " ('Ведущий-Алексей', 0.2443869560956955),\n",
              " ('Недопустимость', 0.24435284733772278),\n",
              " ('2Михаил', 0.23981399834156036),\n",
              " ('лексей', 0.23740756511688232),\n",
              " ('комплексн', 0.23695150017738342),\n",
              " ('персональ', 0.2368222028017044)]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNdYAR1q7uf6"
      },
      "source": [
        "Load small dictionaries for correspoinding words pairs as trainset and testset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "35d_DAK67uf8"
      },
      "source": [
        "def load_word_pairs(filename):\n",
        "    uk_ru_pairs = []\n",
        "    uk_vectors = []\n",
        "    ru_vectors = []\n",
        "    with open(filename, \"r\") as inpf:\n",
        "        for line in inpf:\n",
        "            uk, ru = line.rstrip().split(\"\\t\")\n",
        "            if uk not in uk_emb or ru not in ru_emb:\n",
        "                continue\n",
        "            uk_ru_pairs.append((uk, ru))\n",
        "            uk_vectors.append(uk_emb[uk])\n",
        "            ru_vectors.append(ru_emb[ru])\n",
        "    return uk_ru_pairs, np.array(uk_vectors), np.array(ru_vectors)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wkNL602WHJyO",
        "outputId": "39401d72-66af-4dbf-ca66-875224b64657"
      },
      "source": [
        "!wget -O ukr_rus.train.txt http://tiny.cc/jfgecz"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-10-15 08:49:51--  http://tiny.cc/jfgecz\n",
            "Resolving tiny.cc (tiny.cc)... 157.245.113.153\n",
            "Connecting to tiny.cc (tiny.cc)|157.245.113.153|:80... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://tiny.cc/jfgecz [following]\n",
            "--2021-10-15 08:49:52--  https://tiny.cc/jfgecz\n",
            "Connecting to tiny.cc (tiny.cc)|157.245.113.153|:443... connected.\n",
            "HTTP request sent, awaiting response... 404 Not Found\n",
            "2021-10-15 08:49:52 ERROR 404: Not Found.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uoclU6JcHCcn"
      },
      "source": [
        "!wget -O ukr_rus.test.txt http://tiny.cc/6zoeez"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "05BqsdSK7ugD"
      },
      "source": [
        "uk_ru_train, X_train, Y_train = load_word_pairs(\"/content/drive/My Drive/ml/ukr_rus.train.txt\")"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQOZw51r7ugL"
      },
      "source": [
        "uk_ru_test, X_test, Y_test = load_word_pairs(\"/content/drive/My Drive/ml/ukr_rus.test.txt\")"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ZBBNvpz7ugQ"
      },
      "source": [
        "## Embedding space mapping (0.3 pts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_Dhk5gL7ugS"
      },
      "source": [
        "Let $x_i \\in \\mathrm{R}^d$ be the distributed representation of word $i$ in the source language, and $y_i \\in \\mathrm{R}^d$ is the vector representation of its translation. Our purpose is to learn such linear transform $W$ that minimizes euclidian distance between $Wx_i$ and $y_i$ for some subset of word embeddings. Thus we can formulate so-called Procrustes problem:\n",
        "\n",
        "$$W^*= \\arg\\min_W \\sum_{i=1}^n||Wx_i - y_i||_2$$\n",
        "or\n",
        "$$W^*= \\arg\\min_W ||WX - Y||_F$$\n",
        "\n",
        "where $||*||_F$ - Frobenius norm."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "acOjDdtL7ugY"
      },
      "source": [
        "$W^*= \\arg\\min_W \\sum_{i=1}^n||Wx_i - y_i||_2$ looks like simple multiple linear regression (without intercept fit). So let's code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lb-KN1be7uga",
        "outputId": "3c7bbc62-e17b-4c92-fb3e-aa5a62037e3c"
      },
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# YOUR CODE HERE\n",
        "mapping = LinearRegression(fit_intercept=False)\n",
        "mapping.fit(X_train, Y_train)\n",
        "# -------"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LinearRegression(copy_X=True, fit_intercept=False, n_jobs=None, normalize=False)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7tqJwoY7ugf"
      },
      "source": [
        "Let's take a look at neigbours of the vector of word _\"серпень\"_ (_\"август\"_ in Russian) after linear transform."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "31SrFSbn7ugi",
        "outputId": "1181fb38-72b1-4eb2-895c-8929950847dd"
      },
      "source": [
        "august = mapping.predict(uk_emb[\"серпень\"].reshape(1, -1))\n",
        "ru_emb.most_similar(august)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('апрель', 0.8531432747840881),\n",
              " ('июнь', 0.8402522802352905),\n",
              " ('март', 0.8385884165763855),\n",
              " ('сентябрь', 0.8331484794616699),\n",
              " ('февраль', 0.8311208486557007),\n",
              " ('октябрь', 0.8278019428253174),\n",
              " ('ноябрь', 0.8243728280067444),\n",
              " ('июль', 0.8229618072509766),\n",
              " ('август', 0.8112280368804932),\n",
              " ('январь', 0.8022986650466919)]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "okSkjk597ugo"
      },
      "source": [
        "We can see that neighbourhood of this embedding cosists of different months, but right variant is on the ninth place."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o2uY6Y9B7ugt"
      },
      "source": [
        "As quality measure we will use precision top-1, top-5 and top-10 (for each transformed Ukrainian embedding we count how many right target pairs are found in top N nearest neighbours in Russian embedding space)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zptuho8LAfIE"
      },
      "source": [
        "def precision(pairs, mapped_vectors, topn=1):\n",
        "    \"\"\"\n",
        "    :args:\n",
        "        pairs = list of right word pairs [(uk_word_0, ru_word_0), ...]\n",
        "        mapped_vectors = list of embeddings after mapping from source embedding space to destination embedding space\n",
        "        topn = the number of nearest neighbours in destination embedding space to choose from\n",
        "    :returns:\n",
        "        precision_val, float number, total number of words for those we can find right translation at top K.\n",
        "    \"\"\"\n",
        "    assert len(pairs) == len(mapped_vectors)\n",
        "    num_matches = 0\n",
        "    for i, (_, ru) in enumerate(pairs):\n",
        "        # YOUR CODE HERE\n",
        "        num_matches += int(ru in [j[0] for j in ru_emb.most_similar([mapped_vectors[i]], topn=topn)])\n",
        "    precision_val = num_matches / len(pairs)\n",
        "    return precision_val"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "duhj9hpv7ugy"
      },
      "source": [
        "assert precision([(\"серпень\", \"август\")], august, topn=5) == 0.0\n",
        "assert precision([(\"серпень\", \"август\")], august, topn=9) == 1.0\n",
        "assert precision([(\"серпень\", \"август\")], august, topn=10) == 1.0"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-iyd5gP7ug5"
      },
      "source": [
        "assert precision(uk_ru_test, X_test) == 0.0\n",
        "assert precision(uk_ru_test, Y_test) == 1.0"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U-ssEJ3x7uhA"
      },
      "source": [
        "precision_top1 = precision(uk_ru_test, mapping.predict(X_test), 1)\n",
        "precision_top5 = precision(uk_ru_test, mapping.predict(X_test), 5)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7K-hy7a6Ksn2",
        "outputId": "1e643272-7815-4914-a8b5-5d6e6eb86e62"
      },
      "source": [
        "print(precision_top1)\n",
        "print(precision_top5)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.628498727735369\n",
            "0.7913486005089059\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oa-st_5k7bxt"
      },
      "source": [
        "**Conclusion:** for top1 this method isn't very accurate."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hf6Ou8bx7uhH"
      },
      "source": [
        "## Making it better (orthogonal Procrustean problem) (0.3 pts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4oLs-drN7uhK"
      },
      "source": [
        "It can be shown (see original paper) that a self-consistent linear mapping between semantic spaces should be orthogonal. \n",
        "We can restrict transform $W$ to be orthogonal. Then we will solve next problem:\n",
        "\n",
        "$$W^*= \\arg\\min_W ||WX - Y||_F \\text{, where: } W^TW = I$$\n",
        "\n",
        "$$I \\text{- identity matrix}$$\n",
        "\n",
        "Instead of making yet another regression problem we can find optimal orthogonal transformation using singular value decomposition. It turns out that optimal transformation $W^*$ can be expressed via SVD components:\n",
        "$$X^TY=U\\Sigma V^T\\text{, singular value decompostion}$$\n",
        "$$W^*=UV^T$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_KSaRJFGMFiJ"
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DdFQ7qti7uhL"
      },
      "source": [
        "def learn_transform(X_train, Y_train):\n",
        "    \"\"\" \n",
        "    :returns: W* : float matrix[emb_dim x emb_dim] as defined in formulae above\n",
        "    \"\"\"\n",
        "    # YOUR CODE GOES HERE\n",
        "    # compute orthogonal embedding space mapping\n",
        "    u, s, vh = np.linalg.svd(X_train.T @ Y_train)\n",
        "    mapping = u @ vh\n",
        "\n",
        "    return mapping"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7X7QfYDd7uhQ"
      },
      "source": [
        "W = learn_transform(X_train, Y_train)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OVOFYYa37uhX",
        "outputId": "36362496-ac32-41ef-fe71-f941174ce8d6"
      },
      "source": [
        "ru_emb.most_similar([np.matmul(uk_emb[\"серпень\"], W)])"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('апрель', 0.8245131969451904),\n",
              " ('июнь', 0.805662989616394),\n",
              " ('сентябрь', 0.8055761456489563),\n",
              " ('март', 0.8032935261726379),\n",
              " ('октябрь', 0.7987102270126343),\n",
              " ('июль', 0.7946797013282776),\n",
              " ('ноябрь', 0.7939636707305908),\n",
              " ('август', 0.7938188910484314),\n",
              " ('февраль', 0.7923861145973206),\n",
              " ('декабрь', 0.7715375423431396)]"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r297sYP37uhb",
        "outputId": "7587dc20-84d0-491b-af1a-66c738ca4e9f"
      },
      "source": [
        "print(precision(uk_ru_test, np.matmul(X_test, W)))\n",
        "print(precision(uk_ru_test, np.matmul(X_test, W), 5))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.6437659033078881\n",
            "0.7989821882951654\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nK5KrPEg75wi"
      },
      "source": [
        "**Conclusion:** accuracy hasn't improved very much."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hvUZ72U5AfJg"
      },
      "source": [
        "## Unsupervised embedding-based MT (0.4 pts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LLyuVfHBLrJn"
      },
      "source": [
        "Now, let's build our word embeddings-based translator!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tPAURW1CMuP7"
      },
      "source": [
        "Firstly, download OPUS Tatoeba corpus."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F80kUKzQMsDu",
        "outputId": "7cd8fee8-546f-4fd9-bf42-0176775cf71a"
      },
      "source": [
        "!wget https://object.pouta.csc.fi/OPUS-Tatoeba/v20190709/mono/uk.txt.gz"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-10-17 15:02:30--  https://object.pouta.csc.fi/OPUS-Tatoeba/v20190709/mono/uk.txt.gz\n",
            "Resolving object.pouta.csc.fi (object.pouta.csc.fi)... 86.50.254.18, 86.50.254.19\n",
            "Connecting to object.pouta.csc.fi (object.pouta.csc.fi)|86.50.254.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1819128 (1.7M) [application/gzip]\n",
            "Saving to: ‘uk.txt.gz’\n",
            "\n",
            "uk.txt.gz           100%[===================>]   1.73M  7.90MB/s    in 0.2s    \n",
            "\n",
            "2021-10-17 15:02:30 (7.90 MB/s) - ‘uk.txt.gz’ saved [1819128/1819128]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0CGFZoxCUVf1"
      },
      "source": [
        "!gzip -d ./uk.txt.gz"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2MV3VvoVUX5U"
      },
      "source": [
        "with open('./uk.txt', 'r') as f:\n",
        "    uk_corpus = f.readlines()"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tU7nPVf0UhbI"
      },
      "source": [
        "# To save your time and CPU, feel free to use first 1000 sentences of the corpus\n",
        "uk_corpus = uk_corpus[:1000]"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FLN8dBOXAfJ1",
        "outputId": "86b18d21-0469-4b58-86f5-5dcb2fd60a78"
      },
      "source": [
        "# Any necessary preprocessing if needed\n",
        "\n",
        "uk_corpus[:30]"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Я вже закінчу коледж, коли ви вернетеся з Америки.\\n',\n",
              " 'Він наказав мені негайно вийти з кімнати.\\n',\n",
              " 'Як би ти не намагався, ти не вивчиш англійську за два-три місяці.\\n',\n",
              " 'Поки я не подзвонив, він не прийшов.\\n',\n",
              " 'У всесвіті багато галактик.\\n',\n",
              " 'Вона приймає душ щоранку.\\n',\n",
              " 'Неслухняний хлопчик заблукав й оглядався по сторонах.\\n',\n",
              " 'Вона повільно зникала в туманному лісі.\\n',\n",
              " 'Наш літак летів понад хмарами.\\n',\n",
              " 'У Майка є декілька друзів у Флориді.\\n',\n",
              " 'Місто бомбардували ворожі літаки.\\n',\n",
              " 'Я зустрінуся з тобою в неділю о третій.\\n',\n",
              " 'Для фінансування війни було видано облігації.\\n',\n",
              " 'Звідки беруть початок Олімпійські ігри?\\n',\n",
              " 'Я сказав собі: «Це гарна ідея».\\n',\n",
              " 'Ми збиралися пробути там біля двох тижнів.\\n',\n",
              " 'Як на мене, то наразі помовчу.\\n',\n",
              " 'Мій дядько вчора помер від раку шлунку.\\n',\n",
              " 'Здається, діти втомилися від плавання.\\n',\n",
              " 'Немає кохання без ревнощів.\\n',\n",
              " 'Можливо, я антисоціальний, але це не означає, що я не спілкуюся з людьми.\\n',\n",
              " 'Я не знаю, що ще можна зробити.\\n',\n",
              " 'Я навчився жити без неї.\\n',\n",
              " 'Справді?\\n',\n",
              " 'Мені завжди більше подобалися загадкові персонажі.\\n',\n",
              " 'Я можу тільки чекати.\\n',\n",
              " 'Тобі краще поспати.\\n',\n",
              " 'Обдумай це.\\n',\n",
              " 'Наприклад, тобі подобається англійська?\\n',\n",
              " 'Коли Вам буде зручно?\\n']"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kwZYD3yv44-N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5317ad50-05f5-4f17-e8fb-0d34ef57a492"
      },
      "source": [
        "import re\n",
        "\n",
        "def str2words(text):\n",
        "    text = text.lower()\n",
        "    #text = re.sub(r'[^ґїієа-я\\s\\d]', '', text)\n",
        "    text = re.sub('\\\\n', '', text)\n",
        "    text = re.findall(r\"[ґїієа-я]+|\\S\", text)\n",
        "    return text\n",
        "\n",
        "print(str2words(uk_corpus[0]))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['я', 'вже', 'закінчу', 'коледж', ',', 'коли', 'ви', 'вернетеся', 'з', 'америки', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FGksC7l_NMi9"
      },
      "source": [
        "def translate(sentence):\n",
        "    \"\"\"\n",
        "    :args:\n",
        "        sentence - sentence in Ukrainian (str)\n",
        "    :returns:\n",
        "        translation - sentence in Russian (str)\n",
        "\n",
        "    * find ukrainian embedding for each word in sentence\n",
        "    * transform ukrainian embedding vector\n",
        "    * find nearest russian word and replace\n",
        "    \"\"\"\n",
        "    words = str2words(sentence)\n",
        "    translated = []\n",
        "    for word in words:\n",
        "        if word in uk_emb:\n",
        "            translated.append(ru_emb.most_similar([np.matmul(uk_emb[word], W)], topn=1)[0][0])\n",
        "        else:\n",
        "            translated.append('UNK')\n",
        "    return \" \".join(translated)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4hbbMy-tNxlf"
      },
      "source": [
        "assert translate(\".\") == \".\"\n",
        "assert translate(\"1 , 3\") == \"1 , 3\"\n",
        "assert translate(\"кіт зловив мишу\") == \"кот поймал мышку\""
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ia6I2ce7O_HI"
      },
      "source": [
        "Now you can play with your model and try to get as accurate translations as possible. **Note**: one big issue is out-of-vocabulary words. Try to think of various ways of handling it (you can start with translating each of them to a special **UNK** token and then move to more sophisticated approaches). Good luck!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ap1W7ZCeOAVU",
        "outputId": "de466c0a-e365-4e7c-8543-861282e84e34"
      },
      "source": [
        "for sent in uk_corpus[::10]:\n",
        "    print('uk: ', sent[:-1])\n",
        "    print('ru: ', translate(sent), '\\n')"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "uk:  Я вже закінчу коледж, коли ви вернетеся з Америки.\n",
            "ru:  мной уже закончу колледж , когда мы прибежишь со америки . \n",
            "\n",
            "uk:  Місто бомбардували ворожі літаки.\n",
            "ru:  город бомбили враждебные самолеты . \n",
            "\n",
            "uk:  Можливо, я антисоціальний, але це не означає, що я не спілкуюся з людьми.\n",
            "ru:  возможно , мной антисоциальный , конечно это не означает , что мной не общаюсь со людьми . \n",
            "\n",
            "uk:  Цього ранку випала роса.\n",
            "ru:  этого утра выпала роса . \n",
            "\n",
            "uk:  Біда не приходить одна.\n",
            "ru:  беда не приходит одна . \n",
            "\n",
            "uk:  Подивися на той дим.\n",
            "ru:  посмотри по тот дым . \n",
            "\n",
            "uk:  Я замовив два гамбургера.\n",
            "ru:  мной заказал два гамбургера . \n",
            "\n",
            "uk:  Я не хотів нікого образити.\n",
            "ru:  мной не хотел никого обидеть . \n",
            "\n",
            "uk:  Гора вкрита снігом.\n",
            "ru:  гора покрыта снегом . \n",
            "\n",
            "uk:  На фотографії в дівчини корона не з золота, а з квітів.\n",
            "ru:  по фотографии во девушки корона не со золота , а со цветов . \n",
            "\n",
            "uk:  У мене є мрія.\n",
            "ru:  во меня То мечта . \n",
            "\n",
            "uk:  Я приїхав у Японію з Китаю.\n",
            "ru:  мной приехал во UNK со китая . \n",
            "\n",
            "uk:  На півночі знаходиться Шотландія; на півдні — Англія; на заході — Уельс; і ще далі на заході — Північна Ірландія.\n",
            "ru:  по север находится UNK ; по юге — англия ; по востоке — англо-саксонский ; и ещe дальше по востоке — северная шотландия . \n",
            "\n",
            "uk:  Його рідна країна — Німеччина.\n",
            "ru:  его родная страна — германия . \n",
            "\n",
            "uk:  Берн — столиця Швейцарії.\n",
            "ru:  Уотертаун — столица ирландии . \n",
            "\n",
            "uk:  Він чекав на нього до десятої години.\n",
            "ru:  он ждал по него к десятой часа . \n",
            "\n",
            "uk:  Ти можеш взяти цю книгу даром.\n",
            "ru:  ты можешь взять ту книгу даром . \n",
            "\n",
            "uk:  Цей роман написав відомий американський письменник.\n",
            "ru:  этот роман сочинил известный американский писатель . \n",
            "\n",
            "uk:  Забронюйте, будьте ласкаві, кімнату біля міжнародного аеропорту в Торонто.\n",
            "ru:  забронировать , будте ласковые , комнату возле международного аэропорта во UNK . \n",
            "\n",
            "uk:  Він знає, що ти його кохаєш?\n",
            "ru:  он знает , что ты его влюбится ? \n",
            "\n",
            "uk:  Я знаю, що ти багатий.\n",
            "ru:  мной знаю , что ты богатый . \n",
            "\n",
            "uk:  Ті, хто все забувають, щасливі.\n",
            "ru:  те , кто всё забывают , счастливые . \n",
            "\n",
            "uk:  В цій річці небезпечно плавати.\n",
            "ru:  во этой реке опасно плавать . \n",
            "\n",
            "uk:  Прийшов, побачив, переміг.\n",
            "ru:  пришел , увидел , победил . \n",
            "\n",
            "uk:  Я ходжу до школи пішки.\n",
            "ru:  мной хожу к школы пешком . \n",
            "\n",
            "uk:  Не твоя справа!\n",
            "ru:  не моя дело ! \n",
            "\n",
            "uk:  Не забудь квиток.\n",
            "ru:  не забудь билет . \n",
            "\n",
            "uk:  Хто він?\n",
            "ru:  кто он ? \n",
            "\n",
            "uk:  Ви будете чай чи каву?\n",
            "ru:  мы будете чай ли кофе ? \n",
            "\n",
            "uk:  Він не піде на пікнік, як і я.\n",
            "ru:  он не пойдет по пикник , как и мной . \n",
            "\n",
            "uk:  Коли Ви народилися?\n",
            "ru:  когда мы родились ? \n",
            "\n",
            "uk:  Це моя улюблена пісня.\n",
            "ru:  это моя любимая песня . \n",
            "\n",
            "uk:  Ми майже сім’я.\n",
            "ru:  мы почти семь со мной . \n",
            "\n",
            "uk:  Який гарний сьогодні місяць!\n",
            "ru:  который красивый сегодня месяц ! \n",
            "\n",
            "uk:  Я проти будь-яких війн.\n",
            "ru:  мной против любой – которых войны . \n",
            "\n",
            "uk:  Поверхня повітряної кулі — неевклідовий простір, тому для неї не виконуються правила евклідової геометрії.\n",
            "ru:  поверхность воздушной шары — UNK пространство , потому для неё не выполняются правила симметрической геометрии . \n",
            "\n",
            "uk:  Кажуть, що американці вважають кількість грошей, яку заробляє людина, мірилом його уміння.\n",
            "ru:  дескать , что американцы считают количество денег , какую зарабатывает женщина , мерилом его умение . \n",
            "\n",
            "uk:  Можна я примірю це плаття?\n",
            "ru:  можно мной UNK это платье ? \n",
            "\n",
            "uk:  Якщо буде гарна погода, ми доберемося туди завтра.\n",
            "ru:  если будет красивая погода , мы доберёмся туда завтра . \n",
            "\n",
            "uk:  Це був злий заєць.\n",
            "ru:  это был злой заяц . \n",
            "\n",
            "uk:  Один, два, три, чотири, п'ять, шість, сім, вісім, дев'ять, десять.\n",
            "ru:  один , два , три , четыре , аш со пять , восемь , семь , восемь , девять со пять , десять . \n",
            "\n",
            "uk:  Хто в любові не знається, той горя не знає.\n",
            "ru:  кто во любви не знает , тот горя не знает . \n",
            "\n",
            "uk:  Його мати хвилюється за нього.\n",
            "ru:  его иметь волнуется за него . \n",
            "\n",
            "uk:  Я поважаю тих, хто старається з усіх сил.\n",
            "ru:  мной уважаю тех , кто старается со всех сил . \n",
            "\n",
            "uk:  Їхня дружба переросла у глибоке кохання.\n",
            "ru:  эта дружба переросла во глубокое любовь . \n",
            "\n",
            "uk:  Кейт п’є багато молока кожен день.\n",
            "ru:  джастин аш со То много молока каждый день . \n",
            "\n",
            "uk:  Він злодій.\n",
            "ru:  он вор . \n",
            "\n",
            "uk:  Шумового забруднення можна було б позбігнути тільки якщо б люди були більш чутливими до навколишнього середовища.\n",
            "ru:  шумового загрязнение можно было бы UNK только если бы люди были более чувствительны к окружающей среды . \n",
            "\n",
            "uk:  Чай з лимоном, будьте ласкаві.\n",
            "ru:  чай со лимоном , будте ласковые . \n",
            "\n",
            "uk:  Не плутай бажання з коханням.\n",
            "ru:  не путать желание со влюбленностью . \n",
            "\n",
            "uk:  Я би з задоволенням написав сотні речень в Tatoeb’і, але в мене є справи.\n",
            "ru:  мной бы со удовольствием сочинил сотни сложноподчинённые во tд ; tд  e- .Р со и , конечно во меня То дела . \n",
            "\n",
            "uk:  Дайте мені філіжанку кави.\n",
            "ru:  дайте мне чашечку кофе . \n",
            "\n",
            "uk:  Але ж ти ніколи мені про це не розповідала!\n",
            "ru:  конечно же ты никогда мне о это не рассказывала ! \n",
            "\n",
            "uk:  У тебе будуть проблеми, якщо твої батьки довідаються.\n",
            "ru:  во тебя будут проблемы , если твои родители узнают . \n",
            "\n",
            "uk:  Запах троянд наповнив кімнату.\n",
            "ru:  запах роз наполнил комнату . \n",
            "\n",
            "uk:  Як у тебе справи?\n",
            "ru:  как во тебя дела ? \n",
            "\n",
            "uk:  Це мої штани.\n",
            "ru:  это мои штаны . \n",
            "\n",
            "uk:  Ні, дякую.\n",
            "ru:  ни , спасибо . \n",
            "\n",
            "uk:  Я не розумію, чому Німеччина перемогла на Євробаченні.\n",
            "ru:  мной не понимаю , почему германия победила по Евровиденье . \n",
            "\n",
            "uk:  Добрий вечір.\n",
            "ru:  хороший вечер . \n",
            "\n",
            "uk:  З юбілеєм Олексія Дударева привітав Президент Білорусі Олександр Лукашенко.\n",
            "ru:  со UNK UNK UNK поприветствовал президент беларуссии борис путина. . \n",
            "\n",
            "uk:  Чумацький шлях — широкий пояс із далеких зірок, кожна зірка — сонце, таке як наше.\n",
            "ru:  привольный путь — широкий пояс со далеких звёзд , каждая звезда — солнце , такое как наше . \n",
            "\n",
            "uk:  Незвичайно бачити рок-зірок з краваткою!\n",
            "ru:  необычайно видеть рок – звёзд со галстук ! \n",
            "\n",
            "uk:  Усе печиво у формі зірок.\n",
            "ru:  всё печенье во форме звёзд . \n",
            "\n",
            "uk:  Що мені вдягнути — штани чи спідницю?\n",
            "ru:  что мне одеть — штаны ли юбку ? \n",
            "\n",
            "uk:  Гартман Вітвер — відомий львівський скульптор.\n",
            "ru:  UNK UNK — известный московский скульптор . \n",
            "\n",
            "uk:  То був злий кролик.\n",
            "ru:  то был злой кролик . \n",
            "\n",
            "uk:  Можеш взяти будь-який, що тобі до сподоби.\n",
            "ru:  можешь взять любой – который , что тебе к отвратиться . \n",
            "\n",
            "uk:  Звичайно я піду.\n",
            "ru:  конечно мной пойду . \n",
            "\n",
            "uk:  Шовкопряди прядуть кокони.\n",
            "ru:  шелковичные прядут коконы . \n",
            "\n",
            "uk:  Що б ти зробила, якщо б у тебе було, скажім, десять тисяч доларів?\n",
            "ru:  что бы ты сделала , если бы во тебя было , замечу , десять тысяч долларов ? \n",
            "\n",
            "uk:  Він думає, що він хтось, а насправді він ніхто.\n",
            "ru:  он думает , что он кто-то , а действительно он никто . \n",
            "\n",
            "uk:  Вона дуже пишається своєю колекцією марок.\n",
            "ru:  она очень гордится своею коллекцией марок . \n",
            "\n",
            "uk:  Він дуже простий...\n",
            "ru:  он очень простой . . . \n",
            "\n",
            "uk:  Яка ти добра!\n",
            "ru:  она ты добра ! \n",
            "\n",
            "uk:  Як я за тобою скучив!\n",
            "ru:  как мной за тобой соскучился ! \n",
            "\n",
            "uk:  Це все, що я знаю.\n",
            "ru:  это всё , что мной знаю . \n",
            "\n",
            "uk:  Ти ведеш щоденник?\n",
            "ru:  ты ведёшь дневник ? \n",
            "\n",
            "uk:  Тобі вирішувати.\n",
            "ru:  тебе решать . \n",
            "\n",
            "uk:  Це пошта, а то — банк.\n",
            "ru:  это почта , а то — банк . \n",
            "\n",
            "uk:  Це все, що я хочу зробити.\n",
            "ru:  это всё , что мной хочу сделать . \n",
            "\n",
            "uk:  Я вперше дивлюся такий страшний фільм.\n",
            "ru:  мной впервые смотрю такой страшный фильм . \n",
            "\n",
            "uk:  Ця пісня нагадує мені про дім.\n",
            "ru:  та песня напоминает мне о дом . \n",
            "\n",
            "uk:  Хіросі тут?\n",
            "ru:  UNK здесь ? \n",
            "\n",
            "uk:  Мене звуть Джек.\n",
            "ru:  меня зовут джэк . \n",
            "\n",
            "uk:  Як людина живе, так вона і помре.\n",
            "ru:  как женщина живет , так она и умрет . \n",
            "\n",
            "uk:  Я тут уже дві години.\n",
            "ru:  мной здесь уже две часа . \n",
            "\n",
            "uk:  Мені треба вибачитись перед Ен.\n",
            "ru:  мне надо извиниться перед ен . \n",
            "\n",
            "uk:  Сьогодні я бачив шпака.\n",
            "ru:  сегодня мной видел скворца . \n",
            "\n",
            "uk:  «Скільки коштує ця носова хусточка?» — «Дев'яносто п'ять центів».\n",
            "ru:  « сколько стоить та носовая косыночка ? » — « девять со двадцать аш со пять центов » . \n",
            "\n",
            "uk:  Ранені ведмеді, як правило, дуже небезпечні.\n",
            "ru:  раненные медведи , как правило , очень опасные . \n",
            "\n",
            "uk:  Він швидко втомлюється.\n",
            "ru:  он быстро устает . \n",
            "\n",
            "uk:  Усі готові.\n",
            "ru:  все готовы . \n",
            "\n",
            "uk:  Він скучає по своїй сім'ї.\n",
            "ru:  он скучает по своей семь со мье . \n",
            "\n",
            "uk:  «Дякую», — «На здоров'я».\n",
            "ru:  « спасибо » , — « по здоровье со мной » . \n",
            "\n",
            "uk:  Я ще не знаю своєї адреси, я певний час буду жити в подруги.\n",
            "ru:  мной ещe не знаю своего адреса , мной определенный момент буду жить во подруги . \n",
            "\n",
            "uk:  Амазонка— друга по довжині ріка в світі після Ніла.\n",
            "ru:  амазонка — вторая по длине река во мире после трепещущая . \n",
            "\n",
            "uk:  А якщо побачиш Тома, передай йому від мене вітання.\n",
            "ru:  а если увидишь тома , передай ему от меня поздравления . \n",
            "\n",
            "uk:  Закрий за собою двері.\n",
            "ru:  закрой за собой дверь . \n",
            "\n",
            "uk:  Тримай при собі словник.\n",
            "ru:  держи при себе словарь . \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rKlY1IRmGmE6"
      },
      "source": [
        "Ok, let's see how many there are out-of-vocabulary words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Otog9yWuJAEZ"
      },
      "source": [
        "ooc = set(filter(lambda x: x not in uk_emb, str2words(' '.join(uk_corpus))))\n"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xkGuZFP1Lfai",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6796796f-0f40-4796-e0a4-23da92e5db4b"
      },
      "source": [
        "print('Count of OOC words: ', len(ooc))\n",
        "print(ooc)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Count of OOC words:  79\n",
            "{'голландії', 'прокомпостуйте', 'орлеані', 'вітвер', 'шекспіра', 'рейн', 'поїдзка', 'савако', 'неевклідовий', 'британію', 'індонезії', 'хіросі', 'товада', 'відремонтуй', 'ганусина', 'найабсурдніша', 'позбігнути', 'спростуваті', 'іспанією', 'минилу', 'дівчіна', 'рушді', 'нідерландами', 'атланті', 'італією', 'амфітрити', 'гегеля', 'тед', 'айріс', 'подобавляти', 'чікаґо', 'розхльобуй', 'пікассо', 'дударева', 'юбілеєм', 'терезою', 'олексія', 'джорджія', 'мехіко', 'гартман', 'флориді', 'трасянку', 'кеном', 'мюріел', 'кіото', 'коштовнішого', 'шанхаї', 'венеція', 'примірю', 'краплистою', 'мідорі', 'англійськіх', 'торонто', 'нюношку', 'уельсі', 'хіроші', 'зневолення', 'китаєм', 'танабата', 'вчитимуся', 'діани', 'ятьдесять', 'німеччиною', 'міллера', 'сьюзен', 'нюношка', 'лондону', 'японію', 'маюко', 'пекін', 'салмана', 'кюсю', 'скучатиму', 'прибиральня', 'платон', 'удамо', 'ниряємо', 'шотландія', 'мініспідниці'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ydv3SsxL6QC"
      },
      "source": [
        "Not great, not terrible.\n",
        "One of the possible approaches is to compose a word vector from vectors for its subwords. For example, word <*grey*> is consists of <*gr, gre, rey, ey*>.  Then, in addition to the vector for this word, we also use vectors for character n-grams (which are also in the vocabulary).\n",
        "\n",
        "This changes only the way we form word vector; the whole training pipeline is the same as in the standard Word2Vec. \n",
        "\n",
        "This implemented in fasttext library."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "idktwlcj2xNV",
        "outputId": "c185a91d-6c77-4bec-f24f-2f8bf9965407"
      },
      "source": [
        "pip install fasttext"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fasttext\n",
            "  Downloading fasttext-0.9.2.tar.gz (68 kB)\n",
            "\u001b[?25l\r\u001b[K     |████▊                           | 10 kB 23.2 MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 20 kB 27.2 MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 30 kB 32.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 40 kB 23.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 51 kB 18.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 61 kB 16.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 68 kB 4.4 MB/s \n",
            "\u001b[?25hCollecting pybind11>=2.2\n",
            "  Using cached pybind11-2.8.0-py2.py3-none-any.whl (207 kB)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from fasttext) (57.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from fasttext) (1.19.5)\n",
            "Building wheels for collected packages: fasttext\n",
            "  Building wheel for fasttext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.2-cp37-cp37m-linux_x86_64.whl size=3126415 sha256=31b4880e47d74f86701b8ca79c4dbf42665cbfab406bf8257a474dbdbd426502\n",
            "  Stored in directory: /root/.cache/pip/wheels/4e/ca/bf/b020d2be95f7641801a6597a29c8f4f19e38f9c02a345bab9b\n",
            "Successfully built fasttext\n",
            "Installing collected packages: pybind11, fasttext\n",
            "Successfully installed fasttext-0.9.2 pybind11-2.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w7tYT3WOMh_u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dae32a1c-845e-4ea1-d458-89531a87a1fe"
      },
      "source": [
        "import fasttext\n",
        "ft = fasttext.load_model(\"/content/drive/My Drive/ml/cc.uk.300.bin\")"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "radly2SS66NS"
      },
      "source": [
        "Let's see on the example the difference between word2vec and fasttext. The word **голландії** is not in the vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ayF7A1YOZjj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "4e18cd69-aa24-428e-a298-3de96cb5ab2c"
      },
      "source": [
        "ru_emb.most_similar([np.matmul(ft['голландії'], W)], topn = 1)[0][0]"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'голландии'"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "id": "F8_w-Evg380C",
        "outputId": "44e706e0-3f93-4fdc-fbab-75a26940a480"
      },
      "source": [
        "ru_emb.most_similar([np.matmul(uk_emb['голландії'], W)], topn = 1)[0][0]"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-54-f09063c0a239>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mru_emb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muk_emb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'голландії'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, entities)\u001b[0m\n\u001b[1;32m    335\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentities\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m             \u001b[0;31m# allow calls like trained_model['office'], as a shorthand for trained_model[['office']]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 337\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentities\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentity\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mentity\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentities\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mget_vector\u001b[0;34m(self, word)\u001b[0m\n\u001b[1;32m    453\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 455\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwords_closer_than\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mword_vec\u001b[0;34m(self, word, use_norm)\u001b[0m\n\u001b[1;32m    450\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 452\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"word '%s' not in vocabulary\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    453\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: \"word 'голландії' not in vocabulary\""
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0_-9o1gT4Fwg"
      },
      "source": [
        "def better_translate(sentence):\n",
        "    \"\"\"\n",
        "    :args:\n",
        "        sentence - sentence in Ukrainian (str)\n",
        "    :returns:\n",
        "        translation - sentence in Russian (str)\n",
        "\n",
        "    * find ukrainian embedding for each word in sentence\n",
        "    * transform ukrainian embedding vector\n",
        "    * find nearest russian word and replace\n",
        "    \"\"\"\n",
        "    words = str2words(sentence)\n",
        "    translated = []\n",
        "    for word in words:\n",
        "        translated.append(ru_emb.most_similar([np.matmul(ft[word], W)], topn=1)[0][0])\n",
        "    return \" \".join(translated)"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w0YnonxX4YvY",
        "outputId": "618e6914-881a-42e6-d096-23e526511613"
      },
      "source": [
        "for sent in uk_corpus[::10]:\n",
        "    print('uk: ', sent[:-1])\n",
        "    print('ru: ', better_translate(sent), '\\n')"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "uk:  Я вже закінчу коледж, коли ви вернетеся з Америки.\n",
            "ru:  мной уже закончу колледж , когда мы прибежишь со америки . \n",
            "\n",
            "uk:  Місто бомбардували ворожі літаки.\n",
            "ru:  город бомбили враждебные самолеты . \n",
            "\n",
            "uk:  Можливо, я антисоціальний, але це не означає, що я не спілкуюся з людьми.\n",
            "ru:  возможно , мной антисоциальный , конечно это не означает , что мной не общаюсь со людьми . \n",
            "\n",
            "uk:  Цього ранку випала роса.\n",
            "ru:  этого утра выпала роса . \n",
            "\n",
            "uk:  Біда не приходить одна.\n",
            "ru:  беда не приходит одна . \n",
            "\n",
            "uk:  Подивися на той дим.\n",
            "ru:  посмотри по тот дым . \n",
            "\n",
            "uk:  Я замовив два гамбургера.\n",
            "ru:  мной заказал два гамбургера . \n",
            "\n",
            "uk:  Я не хотів нікого образити.\n",
            "ru:  мной не хотел никого обидеть . \n",
            "\n",
            "uk:  Гора вкрита снігом.\n",
            "ru:  гора покрыта снегом . \n",
            "\n",
            "uk:  На фотографії в дівчини корона не з золота, а з квітів.\n",
            "ru:  по фотографии во девушки корона не со золота , а со цветов . \n",
            "\n",
            "uk:  У мене є мрія.\n",
            "ru:  во меня То мечта . \n",
            "\n",
            "uk:  Я приїхав у Японію з Китаю.\n",
            "ru:  мной приехал во австралию со китая . \n",
            "\n",
            "uk:  На півночі знаходиться Шотландія; на півдні — Англія; на заході — Уельс; і ще далі на заході — Північна Ірландія.\n",
            "ru:  по север находится СибирскоеУссурийскУсть-АвамУсть-БелаяУсть-ИлимскУсть-КаменогорскУсть-КамчатскУсть-КутУсть-МаяУсть-НераУсть-ОрдынскийУсть-ТаркаУтенаУттаУфаУхтаУштобеУшуайяФалештыФаслейнФастовФеодосияФиладельфияФленсбург-МюрвикФлиссингенФлорештыФоросФредериксхавнФримантлФроловоХаапсалуХабаровскХайфаХандыгаХанкендиХанты-МансийскХарабалиХаргХаровскХарстадХарьковХасавюртХасанХатангаХашуриХвалынскХельсинкиХерсонХилокХимкиХмельницкийХобартХову-АксыХойникиХоконсвернХолмскХорремшехрХорс-фьордХортенХост ; по юге — англия ; по востоке — англо-саксонский ; и ещe дальше по востоке — северная шотландия . \n",
            "\n",
            "uk:  Його рідна країна — Німеччина.\n",
            "ru:  его родная страна — германия . \n",
            "\n",
            "uk:  Берн — столиця Швейцарії.\n",
            "ru:  Уотертаун — столица ирландии . \n",
            "\n",
            "uk:  Він чекав на нього до десятої години.\n",
            "ru:  он ждал по него к десятой часа . \n",
            "\n",
            "uk:  Ти можеш взяти цю книгу даром.\n",
            "ru:  ты можешь взять ту книгу даром . \n",
            "\n",
            "uk:  Цей роман написав відомий американський письменник.\n",
            "ru:  этот роман сочинил известный американский писатель . \n",
            "\n",
            "uk:  Забронюйте, будьте ласкаві, кімнату біля міжнародного аеропорту в Торонто.\n",
            "ru:  забронировать , будте ласковые , комнату возле международного аэропорта во плохово . \n",
            "\n",
            "uk:  Він знає, що ти його кохаєш?\n",
            "ru:  он знает , что ты его влюбится ? \n",
            "\n",
            "uk:  Я знаю, що ти багатий.\n",
            "ru:  мной знаю , что ты богатый . \n",
            "\n",
            "uk:  Ті, хто все забувають, щасливі.\n",
            "ru:  те , кто всё забывают , счастливые . \n",
            "\n",
            "uk:  В цій річці небезпечно плавати.\n",
            "ru:  во этой реке опасно плавать . \n",
            "\n",
            "uk:  Прийшов, побачив, переміг.\n",
            "ru:  пришел , увидел , победил . \n",
            "\n",
            "uk:  Я ходжу до школи пішки.\n",
            "ru:  мной хожу к школы пешком . \n",
            "\n",
            "uk:  Не твоя справа!\n",
            "ru:  не моя дело ! \n",
            "\n",
            "uk:  Не забудь квиток.\n",
            "ru:  не забудь билет . \n",
            "\n",
            "uk:  Хто він?\n",
            "ru:  кто он ? \n",
            "\n",
            "uk:  Ви будете чай чи каву?\n",
            "ru:  мы будете чай ли кофе ? \n",
            "\n",
            "uk:  Він не піде на пікнік, як і я.\n",
            "ru:  он не пойдет по пикник , как и мной . \n",
            "\n",
            "uk:  Коли Ви народилися?\n",
            "ru:  когда мы родились ? \n",
            "\n",
            "uk:  Це моя улюблена пісня.\n",
            "ru:  это моя любимая песня . \n",
            "\n",
            "uk:  Ми майже сім’я.\n",
            "ru:  мы почти семь со мной . \n",
            "\n",
            "uk:  Який гарний сьогодні місяць!\n",
            "ru:  который красивый сегодня месяц ! \n",
            "\n",
            "uk:  Я проти будь-яких війн.\n",
            "ru:  мной против любой – которых войны . \n",
            "\n",
            "uk:  Поверхня повітряної кулі — неевклідовий простір, тому для неї не виконуються правила евклідової геометрії.\n",
            "ru:  поверхность воздушной шары — математический пространство , потому для неё не выполняются правила симметрической геометрии . \n",
            "\n",
            "uk:  Кажуть, що американці вважають кількість грошей, яку заробляє людина, мірилом його уміння.\n",
            "ru:  дескать , что американцы считают количество денег , какую зарабатывает женщина , мерилом его умение . \n",
            "\n",
            "uk:  Можна я примірю це плаття?\n",
            "ru:  можно мной примеряет это платье ? \n",
            "\n",
            "uk:  Якщо буде гарна погода, ми доберемося туди завтра.\n",
            "ru:  если будет красивая погода , мы доберёмся туда завтра . \n",
            "\n",
            "uk:  Це був злий заєць.\n",
            "ru:  это был злой заяц . \n",
            "\n",
            "uk:  Один, два, три, чотири, п'ять, шість, сім, вісім, дев'ять, десять.\n",
            "ru:  один , два , три , четыре , аш со пять , восемь , семь , восемь , девять со пять , десять . \n",
            "\n",
            "uk:  Хто в любові не знається, той горя не знає.\n",
            "ru:  кто во любви не знает , тот горя не знает . \n",
            "\n",
            "uk:  Його мати хвилюється за нього.\n",
            "ru:  его иметь волнуется за него . \n",
            "\n",
            "uk:  Я поважаю тих, хто старається з усіх сил.\n",
            "ru:  мной уважаю тех , кто старается со всех сил . \n",
            "\n",
            "uk:  Їхня дружба переросла у глибоке кохання.\n",
            "ru:  эта дружба переросла во глубокое любовь . \n",
            "\n",
            "uk:  Кейт п’є багато молока кожен день.\n",
            "ru:  джастин аш со То много молока каждый день . \n",
            "\n",
            "uk:  Він злодій.\n",
            "ru:  он вор . \n",
            "\n",
            "uk:  Шумового забруднення можна було б позбігнути тільки якщо б люди були більш чутливими до навколишнього середовища.\n",
            "ru:  шумового загрязнение можно было бы свесить только если бы люди были более чувствительны к окружающей среды . \n",
            "\n",
            "uk:  Чай з лимоном, будьте ласкаві.\n",
            "ru:  чай со лимоном , будте ласковые . \n",
            "\n",
            "uk:  Не плутай бажання з коханням.\n",
            "ru:  не путать желание со влюбленностью . \n",
            "\n",
            "uk:  Я би з задоволенням написав сотні речень в Tatoeb’і, але в мене є справи.\n",
            "ru:  мной бы со удовольствием сочинил сотни сложноподчинённые во tд ; tд  e- .Р со и , конечно во меня То дела . \n",
            "\n",
            "uk:  Дайте мені філіжанку кави.\n",
            "ru:  дайте мне чашечку кофе . \n",
            "\n",
            "uk:  Але ж ти ніколи мені про це не розповідала!\n",
            "ru:  конечно же ты никогда мне о это не рассказывала ! \n",
            "\n",
            "uk:  У тебе будуть проблеми, якщо твої батьки довідаються.\n",
            "ru:  во тебя будут проблемы , если твои родители узнают . \n",
            "\n",
            "uk:  Запах троянд наповнив кімнату.\n",
            "ru:  запах роз наполнил комнату . \n",
            "\n",
            "uk:  Як у тебе справи?\n",
            "ru:  как во тебя дела ? \n",
            "\n",
            "uk:  Це мої штани.\n",
            "ru:  это мои штаны . \n",
            "\n",
            "uk:  Ні, дякую.\n",
            "ru:  ни , спасибо . \n",
            "\n",
            "uk:  Я не розумію, чому Німеччина перемогла на Євробаченні.\n",
            "ru:  мной не понимаю , почему германия победила по Евровиденье . \n",
            "\n",
            "uk:  Добрий вечір.\n",
            "ru:  хороший вечер . \n",
            "\n",
            "uk:  З юбілеєм Олексія Дударева привітав Президент Білорусі Олександр Лукашенко.\n",
            "ru:  со бракосочетанием анатолия Анютина поприветствовал президент беларуссии борис путина. . \n",
            "\n",
            "uk:  Чумацький шлях — широкий пояс із далеких зірок, кожна зірка — сонце, таке як наше.\n",
            "ru:  привольный путь — широкий пояс со далеких звёзд , каждая звезда — солнце , такое как наше . \n",
            "\n",
            "uk:  Незвичайно бачити рок-зірок з краваткою!\n",
            "ru:  необычайно видеть рок – звёзд со галстук ! \n",
            "\n",
            "uk:  Усе печиво у формі зірок.\n",
            "ru:  всё печенье во форме звёзд . \n",
            "\n",
            "uk:  Що мені вдягнути — штани чи спідницю?\n",
            "ru:  что мне одеть — штаны ли юбку ? \n",
            "\n",
            "uk:  Гартман Вітвер — відомий львівський скульптор.\n",
            "ru:  toxunYouMırtaАзербайджанБрэйн четверг — известный московский скульптор . \n",
            "\n",
            "uk:  То був злий кролик.\n",
            "ru:  то был злой кролик . \n",
            "\n",
            "uk:  Можеш взяти будь-який, що тобі до сподоби.\n",
            "ru:  можешь взять любой – который , что тебе к отвратиться . \n",
            "\n",
            "uk:  Звичайно я піду.\n",
            "ru:  конечно мной пойду . \n",
            "\n",
            "uk:  Шовкопряди прядуть кокони.\n",
            "ru:  шелковичные прядут коконы . \n",
            "\n",
            "uk:  Що б ти зробила, якщо б у тебе було, скажім, десять тисяч доларів?\n",
            "ru:  что бы ты сделала , если бы во тебя было , замечу , десять тысяч долларов ? \n",
            "\n",
            "uk:  Він думає, що він хтось, а насправді він ніхто.\n",
            "ru:  он думает , что он кто-то , а действительно он никто . \n",
            "\n",
            "uk:  Вона дуже пишається своєю колекцією марок.\n",
            "ru:  она очень гордится своею коллекцией марок . \n",
            "\n",
            "uk:  Він дуже простий...\n",
            "ru:  он очень простой . . . \n",
            "\n",
            "uk:  Яка ти добра!\n",
            "ru:  она ты добра ! \n",
            "\n",
            "uk:  Як я за тобою скучив!\n",
            "ru:  как мной за тобой соскучился ! \n",
            "\n",
            "uk:  Це все, що я знаю.\n",
            "ru:  это всё , что мной знаю . \n",
            "\n",
            "uk:  Ти ведеш щоденник?\n",
            "ru:  ты ведёшь дневник ? \n",
            "\n",
            "uk:  Тобі вирішувати.\n",
            "ru:  тебе решать . \n",
            "\n",
            "uk:  Це пошта, а то — банк.\n",
            "ru:  это почта , а то — банк . \n",
            "\n",
            "uk:  Це все, що я хочу зробити.\n",
            "ru:  это всё , что мной хочу сделать . \n",
            "\n",
            "uk:  Я вперше дивлюся такий страшний фільм.\n",
            "ru:  мной впервые смотрю такой страшный фильм . \n",
            "\n",
            "uk:  Ця пісня нагадує мені про дім.\n",
            "ru:  та песня напоминает мне о дом . \n",
            "\n",
            "uk:  Хіросі тут?\n",
            "ru:  предутреннем здесь ? \n",
            "\n",
            "uk:  Мене звуть Джек.\n",
            "ru:  меня зовут джэк . \n",
            "\n",
            "uk:  Як людина живе, так вона і помре.\n",
            "ru:  как женщина живет , так она и умрет . \n",
            "\n",
            "uk:  Я тут уже дві години.\n",
            "ru:  мной здесь уже две часа . \n",
            "\n",
            "uk:  Мені треба вибачитись перед Ен.\n",
            "ru:  мне надо извиниться перед ен . \n",
            "\n",
            "uk:  Сьогодні я бачив шпака.\n",
            "ru:  сегодня мной видел скворца . \n",
            "\n",
            "uk:  «Скільки коштує ця носова хусточка?» — «Дев'яносто п'ять центів».\n",
            "ru:  « сколько стоить та носовая косыночка ? » — « девять со двадцать аш со пять центов » . \n",
            "\n",
            "uk:  Ранені ведмеді, як правило, дуже небезпечні.\n",
            "ru:  раненные медведи , как правило , очень опасные . \n",
            "\n",
            "uk:  Він швидко втомлюється.\n",
            "ru:  он быстро устает . \n",
            "\n",
            "uk:  Усі готові.\n",
            "ru:  все готовы . \n",
            "\n",
            "uk:  Він скучає по своїй сім'ї.\n",
            "ru:  он скучает по своей семь со мье . \n",
            "\n",
            "uk:  «Дякую», — «На здоров'я».\n",
            "ru:  « спасибо » , — « по здоровье со мной » . \n",
            "\n",
            "uk:  Я ще не знаю своєї адреси, я певний час буду жити в подруги.\n",
            "ru:  мной ещe не знаю своего адреса , мной определенный момент буду жить во подруги . \n",
            "\n",
            "uk:  Амазонка— друга по довжині ріка в світі після Ніла.\n",
            "ru:  амазонка — вторая по длине река во мире после трепещущая . \n",
            "\n",
            "uk:  А якщо побачиш Тома, передай йому від мене вітання.\n",
            "ru:  а если увидишь тома , передай ему от меня поздравления . \n",
            "\n",
            "uk:  Закрий за собою двері.\n",
            "ru:  закрой за собой дверь . \n",
            "\n",
            "uk:  Тримай при собі словник.\n",
            "ru:  держи при себе словарь . \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PrxVM7GDV_EZ"
      },
      "source": [
        "Great! \n",
        "See second notebook for the Neural Machine Translation assignment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55QC5cry8Pd_"
      },
      "source": [
        "**Conclusion:** for such a simple translator the result isn't as bad as it seems. However there're a lot of flaws. Not accurate translation, if we use only top1, and the problem with out-of-vocabulary words, which can be solved by using fasttext  instead of word2vectors."
      ]
    }
  ]
}